{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRTeUw2yBj9C",
        "outputId": "066681a9-33dd-4742-f8e2-f39d164a6ca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "import math\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUUkoUduuUMi",
        "outputId": "6d7bd1d5-62da-40d5-b4de-1f8305ab9964"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_cardocuments(path):\n",
        "\n",
        "    content = {}\n",
        "    doc_id_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            with open(os.path.join(path, filename), 'r', encoding='utf-8') as file:\n",
        "                content[doc_id] = file.read()\n",
        "                doc_id_filename[doc_id] = filename\n",
        "                logging.info(f\"Loaded file: {filename} with doc_id: {doc_id}\")\n",
        "                doc_id += 1\n",
        "    return content, doc_id_filename"
      ],
      "metadata": {
        "id": "M_t7gn4TBn-w"
      },
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "\n",
        "    if isinstance(text, str):\n",
        "        return text.lower().split()\n",
        "    else:\n",
        "        return []"
      ],
      "metadata": {
        "id": "EahsiyCqBrO-"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def term_frequency(term, document):\n",
        "    return document.count(term) / len(document)"
      ],
      "metadata": {
        "id": "SLU2XOnPBuN9"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_document_frequency(term, all_documents):\n",
        "    num_docs_containing_term = sum(1 for doc in all_documents if term in doc)\n",
        "    return math.log(len(all_documents) / (1 + num_docs_containing_term))"
      ],
      "metadata": {
        "id": "-Ue8rLMOBvqT"
      },
      "execution_count": 279,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(document, all_documents, vocab):\n",
        "    tfidf_vector = []\n",
        "    for term in vocab:\n",
        "        tf = term_frequency(term, document)\n",
        "        idf = inverse_document_frequency(term, all_documents)\n",
        "        tfidf_vector.append(tf * idf)\n",
        "    return np.array(tfidf_vector)"
      ],
      "metadata": {
        "id": "SDp-j7ClBxE2"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_doc_ids_to_filenames(doc_ids, doc_id_to_filename):\n",
        "    # Indent the return statement to be part of the function body\n",
        "    return [doc_id_to_filename[doc_id] for doc_id in doc_ids]"
      ],
      "metadata": {
        "id": "CoOIi8aPxq98"
      },
      "execution_count": 281,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    # Reshape to 1D if necessary to avoid shape issues\n",
        "    vec1 = vec1.reshape(-1)  # Convert to 1D\n",
        "    vec2 = vec2.reshape(-1)  # Convert to 1D\n",
        "\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2 + 1e-8)"
      ],
      "metadata": {
        "id": "BABwFVyn4JcS"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    return tokens  # Return the list of cleaned tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "P2NheoIP386c"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cars(folder_path):\n",
        "\n",
        "    car_data = {}\n",
        "    doc_id_to_filename = {}\n",
        "    doc_id = 0\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            # Try opening with 'latin-1' encoding\n",
        "            try:\n",
        "                with open(os.path.join(folder_path, filename), 'r', encoding='latin-1') as f:\n",
        "                    content = f.read()\n",
        "            # If 'latin-1' fails, try 'utf-8'\n",
        "            except UnicodeDecodeError:\n",
        "                with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "\n",
        "            car_data[doc_id] = content\n",
        "            doc_id_to_filename[doc_id] = filename\n",
        "            doc_id += 1\n",
        "    return car_data, doc_id_to_filename\n"
      ],
      "metadata": {
        "id": "YVNdqaOO96dr"
      },
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_car_data(text):\n",
        "    \"\"\"Extracts car data (Make, Model, Year, Price, Mileage) from text.\"\"\"\n",
        "    car_data = {}\n",
        "    # Define regex patterns to match car data fields\n",
        "    make_model_pattern = re.compile(r\"(?i)(make|model):\\s*([^\\n,]+)\")\n",
        "    year_pattern = re.compile(r\"(?i)year:\\s*(\\d{4})\")\n",
        "    price_pattern = re.compile(r\"(?i)price:\\s*\\$?([\\d,]+)\")\n",
        "    mileage_pattern = re.compile(r\"(?i)mileage:\\s*([\\d,]+)\")\n",
        "\n",
        "    # Search for matches in the text\n",
        "    make_model_match = make_model_pattern.findall(text)\n",
        "    year_match = year_pattern.search(text)\n",
        "    price_match = price_pattern.search(text)\n",
        "    mileage_match = mileage_pattern.search(text)\n",
        "\n",
        "    # Extract and store the data\n",
        "    for key, value in make_model_match:\n",
        "        car_data[key.lower()] = value.strip()\n",
        "    if year_match:\n",
        "        car_data[\"year\"] = year_match.group(1)\n",
        "    if price_match:\n",
        "        car_data[\"price\"] = price_match.group(1)\n",
        "    if mileage_match:\n",
        "        car_data[\"mileage\"] = mileage_match.group(1)\n",
        "\n",
        "    return car_data"
      ],
      "metadata": {
        "id": "LNg05T5U457g"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_queries(query, all_documents, doc_tfidf_vectors, vocab, top_k=3):\n",
        "\n",
        "    tokenized_query = clean_text(query)\n",
        "    query_vector = compute_tfidf(tokenized_query, all_documents, vocab)\n",
        "    similarities = []\n",
        "    for doc_id, doc_vector in enumerate(doc_tfidf_vectors):\n",
        "        # The cosine_similarity function returns a scalar, so no need to index\n",
        "        similarity = cosine_similarity(query_vector.reshape(1, -1), doc_vector.reshape(1, -1))\n",
        "        similarities.append((doc_id, similarity))\n",
        "\n",
        "    def similarity_comparator(pair):\n",
        "        return pair[1]\n",
        "\n",
        "    similarities.sort(key=similarity_comparator, reverse=True)\n",
        "    return similarities[:top_k]"
      ],
      "metadata": {
        "id": "nhLwEnBa9jTS"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    folder_path = \"/content/drive/MyDrive/Tech400_final_project\"\n",
        "    print(\"Retrieving Car Data...\")\n",
        "    car_data, doc_id_to_filename = load_cars(folder_path)\n",
        "    queries = input(\"Enter your queries: \")\n",
        "\n",
        "    tokenized_docs = [clean_text(doc) for doc in car_data.values()]\n",
        "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "    doc_tfidf_vectors = [compute_tfidf(doc, tokenized_docs, vocab) for doc in tokenized_docs]\n",
        "\n",
        "    print(f\"Searching for the result:'{queries}' \")\n",
        "    similarities = process_queries(queries, tokenized_docs, doc_tfidf_vectors, vocab)\n",
        "    results = [(queries, similarities)]\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nTop 3 Matching Cars:\")\n",
        "        for idx, (doc_id, score) in enumerate(results[0][1], 1):\n",
        "            car_info = extract_car_data(car_data[doc_id])\n",
        "            car_name = os.path.splitext(doc_id_to_filename[doc_id])[0]\n",
        "            print(f\"Car {idx}: {car_name},Score: {score:.4f}\")\n",
        "            # Display additional car details based on extracted information\n",
        "            if 'safety_ratings' in car_info:\n",
        "                print(f\"  Safety Rating: {car_info['safety_ratings']}\")\n",
        "            if 'family_friendly' in car_info:\n",
        "                print(f\" Family-Friendly: Yes\")\n",
        "            if 'suv' in car_info:\n",
        "                print(f\"  SUV: Yes\")\n",
        "            if 'luxury' in car_info:\n",
        "                print(f\"  Luxury: Yes\")\n",
        "    else:\n",
        "        print(\"No matching cars found.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1tNaCAJ_8qZ",
        "outputId": "9cf14e5c-40ae-4d30-cffc-6f95224c4ea3"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving Car Data...\n",
            "Enter your queries: Horsepower\n",
            "Searching for the result:'Horsepower' \n",
            "\n",
            "Top 3 Matching Cars:\n",
            "Car 1: Range Rover Sport SVR ,Score: 0.0047\n",
            "Car 2: Jaguar F-PACE SVR ,Score: 0.0046\n",
            "Car 3: Mercedes-AMG GLE 63 S ,Score: 0.0044\n"
          ]
        }
      ]
    }
  ]
}